{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4VIaOH4sSQF"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docling-project/docling/blob/main/docs/examples/visual_grounding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psPLx3IhsSQG"
      },
      "source": [
        "# Visual grounding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ivKG-SUsSQG"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | ðŸ’» Local |\n",
        "| Vector store | Milvus | ðŸ’» Local |\n",
        "| Gen AI | Hugging Face Inference API | ðŸŒ Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vreD2m0sSQH"
      },
      "source": [
        "This example showcases Docling's **visual grounding** capabilities, which can be combined\n",
        "with any agentic AI / RAG framework.\n",
        "\n",
        "In this instance, we illustrate these capabilities leveraging the\n",
        "[LangChain Docling integration](../../integrations/langchain/), along with a Milvus\n",
        "vector store, as well as sentence-transformers embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiVq1MSsSQH"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7yGGfgXsSQH"
      },
      "source": [
        "- ðŸ‘‰ For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.\n",
        "- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.\n",
        "- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cpk1O1B7sSQH"
      },
      "outputs": [],
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts langchain-docling langchain-core langchain_google_genai langchain-huggingface langchain_milvus langchain matplotlib python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lXWKDdWusSQI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "SOURCES = ['/content/Purdue.pdf'] # transcript\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GEN_MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "QUESTION = \"What is the grade for SC200?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 3\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8zOV12XsSQI"
      },
      "source": [
        "## Document store setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfowZSinsSQJ"
      },
      "source": [
        "## Document loading\n",
        "\n",
        "We first define our converter, in this case including options for keeping page images (for visual grounding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lW_AKOJasSQJ"
      },
      "outputs": [],
      "source": [
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(\n",
        "            pipeline_options=PdfPipelineOptions(\n",
        "                generate_page_images=True,\n",
        "                images_scale=2.0,\n",
        "            ),\n",
        "        )\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn4n51GksSQJ"
      },
      "source": [
        "We set up a simple doc store for keeping converted documents, as that is needed for visual grounding further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EtRbTfydsSQJ"
      },
      "outputs": [],
      "source": [
        "doc_store = {}\n",
        "doc_store_root = Path(mkdtemp())\n",
        "for source in SOURCES:\n",
        "    dl_doc = converter.convert(source=source).document\n",
        "    file_path = Path(doc_store_root / f\"{dl_doc.origin.binary_hash}.json\")\n",
        "    dl_doc.save_as_json(file_path)\n",
        "    doc_store[dl_doc.origin.binary_hash] = file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Has_bwnOsSQJ"
      },
      "source": [
        "Now we can instantiate our loader and load documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7mMW5ygsSQJ",
        "outputId": "6acdd6aa-55b4-4e92-a426-79d51ad81831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4239 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=SOURCES,\n",
        "    converter=converter,\n",
        "    export_type=ExportType.DOC_CHUNKS,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG1fbOEmsSQJ"
      },
      "source": [
        "> ðŸ‘‰ **NOTE**: As you see above, using the `HybridChunker` can sometimes lead to a warning from the transformers library, however this is a \"false alarm\" â€” for details check [here](https://docling-project.github.io/docling/faq/#hybridchunker-triggers-warning-token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length-for-this-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTDogUpOsSQJ"
      },
      "source": [
        "Inspecting some sample splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2EAcyXGsSQJ",
        "outputId": "4250208f-2f12-454f-bad4-d9fbbc62a812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- d.page_content='Office of the Registrar 2550 Northwestern Avenue, Suite 1100 West Lafayette, IN 47906\\nDate Issued: 25-May-2021\\nRecipient:\\nTranscript of:\\nCodi Horst\\nkeeley11.ch@gmail.com'\n",
            "- d.page_content='How to Authenticate this Official Transcript from Purdue University Global\\nThis official transcript has been transmitted electronically to the requested recipient and is intended solely for use by that recipient. If you are not the intended recipient, please notify the Office of the Registrar at Purdue University Global. It is not permissible to replicate this document or forward it to any person or organization other than the identified recipient. Release of this record or disclosure of its contents to any third party without written consent of the record owner is prohibited.\\nThis official transcript has been digitally signed and therefore contains special characteristics. If this document has been issued by Purdue University Global, and for optimal results, we recommend that this document is viewed with the latest version of AdobeÂ® Acrobat or AdobeÂ® Reader; it will reveal a digital certificate that has been applied to the transcript. This digital certificate will appear in a pop-up screen or status bar on the document, display a blue ribbon, and declare that the document was certified by Purdue University Global with a valid certificate issued by GeoTrust CA for AdobeÂ®. This document certification can be validated by clicking on the Signature Properties of the document.'\n",
            "- d.page_content='How to Authenticate this Official Transcript from Purdue University Global\\nThe blue ribbon symbol is your assurance that the digital certificate is valid, the document is authentic, and the contents of the transcript have not been altered.\\nIf the transcript does not display a valid certification and signature message, reject this transcript immediately.  An invalid digital certificate display means either the digital signature is not authentic, or the document has been altered.  The digital signature can also be revoked by the Office of the Registrar if there is cause, and digital signatures can expire.  A document with an invalid digital signature display should be rejected.\\nLastly, one other possible message, Author Unknown, can have two possible meanings: The certificate is a self-signed certificate or has been issued by an unknown or untrusted certificate authority and therefore has not been trusted, or the revocation check could not complete. If you receive this message make sure you are properly connected to the internet. If you have a connection and you still cannot validate the digital certificate on-line, reject this document.\\nThe transcript key is the last page of this document.\\nThe current version of AdobeÂ® Reader is free of charge, and available for immediate download at http://www.adobe.com.'\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "for d in docs[:3]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMIsb4EjsSQJ"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8db-wsWnsSQJ",
        "outputId": "6e2416eb-3fdb-4d11-cc4d-d876f8cae14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9p_VB1AsSQJ"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wJzarr0KsSQJ"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Existing Milvus vectorstore assumed to be initialized as provided\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key='')\n",
        "\n",
        "# Create retrieval and document chain\n",
        "retrieval_chain = create_retrieval_chain(\n",
        "    retriever,\n",
        "    create_stuff_documents_chain(llm, prompt=PROMPT)\n",
        ")\n",
        "\n",
        "def clip_text(text, threshold=100):\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ehiXjvuavSb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-D8C5hWsSQJ",
        "outputId": "839791b9-e48e-4ef5-c33d-b17eccb4a2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "What is the grade for SC200?\n",
            "\n",
            "Answer:\n",
            "The provided context information mentions \"SC200\" under \"Purdue University Global\", but it does not specify a grade for it.\n"
          ]
        }
      ],
      "source": [
        "from docling.chunking import DocMeta\n",
        "from docling.datamodel.document import DoclingDocument\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMw9wn9sSQJ"
      },
      "source": [
        "### Visual grounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8XmpegMsSQJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import ImageDraw\n",
        "\n",
        "for i, doc in enumerate(resp_dict[\"context\"][:]):\n",
        "    image_by_page = {}\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    meta = DocMeta.model_validate(doc.metadata[\"dl_meta\"])\n",
        "\n",
        "    # loading the full DoclingDocument from the document store:\n",
        "    dl_doc = DoclingDocument.load_from_json(doc_store.get(meta.origin.binary_hash))\n",
        "\n",
        "    for doc_item in meta.doc_items:\n",
        "        if doc_item.prov:\n",
        "            prov = doc_item.prov[0]  # here we only consider the first provenence item\n",
        "            page_no = prov.page_no\n",
        "            if img := image_by_page.get(page_no):\n",
        "                pass\n",
        "            else:\n",
        "                page = dl_doc.pages[prov.page_no]\n",
        "                print(f\"  page: {prov.page_no}\")\n",
        "                img = page.image.pil_image\n",
        "                image_by_page[page_no] = img\n",
        "            bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
        "            bbox = bbox.normalized(page.size)\n",
        "            thickness = 2\n",
        "            padding = thickness + 2\n",
        "            bbox.l = round(bbox.l * img.width - padding)\n",
        "            bbox.r = round(bbox.r * img.width + padding)\n",
        "            bbox.t = round(bbox.t * img.height - padding)\n",
        "            bbox.b = round(bbox.b * img.height + padding)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(\n",
        "                xy=bbox.as_tuple(),\n",
        "                outline=\"blue\",\n",
        "                width=thickness,\n",
        "            )\n",
        "    for p in image_by_page:\n",
        "        img = image_by_page[p]\n",
        "        plt.figure(figsize=[15, 15])\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Bk9aIGifsSQK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}