{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4VIaOH4sSQF"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docling-project/docling/blob/main/docs/examples/visual_grounding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psPLx3IhsSQG"
      },
      "source": [
        "# Visual grounding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ivKG-SUsSQG"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | ðŸ’» Local |\n",
        "| Vector store | Milvus | ðŸ’» Local |\n",
        "| Gen AI | Hugging Face Inference API | ðŸŒ Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vreD2m0sSQH"
      },
      "source": [
        "This example showcases Docling's **visual grounding** capabilities, which can be combined\n",
        "with any agentic AI / RAG framework.\n",
        "\n",
        "In this instance, we illustrate these capabilities leveraging the\n",
        "[LangChain Docling integration](../../integrations/langchain/), along with a Milvus\n",
        "vector store, as well as sentence-transformers embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiVq1MSsSQH"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7yGGfgXsSQH"
      },
      "source": [
        "- ðŸ‘‰ For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.\n",
        "- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.\n",
        "- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpk1O1B7sSQH",
        "outputId": "aad540d8-ef97-4260-c464-80b88f9dbb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts langchain-docling langchain-core langchain-huggingface langchain_milvus langchain matplotlib python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lXWKDdWusSQI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "SOURCES = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GEN_MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "QUESTION = \"Which are the main AI models in Docling?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 3\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8zOV12XsSQI"
      },
      "source": [
        "## Document store setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfowZSinsSQJ"
      },
      "source": [
        "## Document loading\n",
        "\n",
        "We first define our converter, in this case including options for keeping page images (for visual grounding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lW_AKOJasSQJ"
      },
      "outputs": [],
      "source": [
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(\n",
        "            pipeline_options=PdfPipelineOptions(\n",
        "                generate_page_images=True,\n",
        "                images_scale=2.0,\n",
        "            ),\n",
        "        )\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn4n51GksSQJ"
      },
      "source": [
        "We set up a simple doc store for keeping converted documents, as that is needed for visual grounding further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtRbTfydsSQJ"
      },
      "outputs": [],
      "source": [
        "doc_store = {}\n",
        "doc_store_root = Path(mkdtemp())\n",
        "for source in SOURCES:\n",
        "    dl_doc = converter.convert(source=source).document\n",
        "    file_path = Path(doc_store_root / f\"{dl_doc.origin.binary_hash}.json\")\n",
        "    dl_doc.save_as_json(file_path)\n",
        "    doc_store[dl_doc.origin.binary_hash] = file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Has_bwnOsSQJ"
      },
      "source": [
        "Now we can instantiate our loader and load documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7mMW5ygsSQJ"
      },
      "outputs": [],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=SOURCES,\n",
        "    converter=converter,\n",
        "    export_type=ExportType.DOC_CHUNKS,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG1fbOEmsSQJ"
      },
      "source": [
        "> ðŸ‘‰ **NOTE**: As you see above, using the `HybridChunker` can sometimes lead to a warning from the transformers library, however this is a \"false alarm\" â€” for details check [here](https://docling-project.github.io/docling/faq/#hybridchunker-triggers-warning-token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length-for-this-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTDogUpOsSQJ"
      },
      "source": [
        "Inspecting some sample splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2EAcyXGsSQJ"
      },
      "outputs": [],
      "source": [
        "for d in docs[:3]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMIsb4EjsSQJ"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8db-wsWnsSQJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9p_VB1AsSQJ"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJzarr0KsSQJ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=GEN_MODEL_ID,\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "\n",
        "def clip_text(text, threshold=100):\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ehiXjvuavSb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-D8C5hWsSQJ"
      },
      "outputs": [],
      "source": [
        "from docling.chunking import DocMeta\n",
        "from docling.datamodel.document import DoclingDocument\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMw9wn9sSQJ"
      },
      "source": [
        "### Visual grounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8XmpegMsSQJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import ImageDraw\n",
        "\n",
        "for i, doc in enumerate(resp_dict[\"context\"][:]):\n",
        "    image_by_page = {}\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
        "    meta = DocMeta.model_validate(doc.metadata[\"dl_meta\"])\n",
        "\n",
        "    # loading the full DoclingDocument from the document store:\n",
        "    dl_doc = DoclingDocument.load_from_json(doc_store.get(meta.origin.binary_hash))\n",
        "\n",
        "    for doc_item in meta.doc_items:\n",
        "        if doc_item.prov:\n",
        "            prov = doc_item.prov[0]  # here we only consider the first provenence item\n",
        "            page_no = prov.page_no\n",
        "            if img := image_by_page.get(page_no):\n",
        "                pass\n",
        "            else:\n",
        "                page = dl_doc.pages[prov.page_no]\n",
        "                print(f\"  page: {prov.page_no}\")\n",
        "                img = page.image.pil_image\n",
        "                image_by_page[page_no] = img\n",
        "            bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
        "            bbox = bbox.normalized(page.size)\n",
        "            thickness = 2\n",
        "            padding = thickness + 2\n",
        "            bbox.l = round(bbox.l * img.width - padding)\n",
        "            bbox.r = round(bbox.r * img.width + padding)\n",
        "            bbox.t = round(bbox.t * img.height - padding)\n",
        "            bbox.b = round(bbox.b * img.height + padding)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(\n",
        "                xy=bbox.as_tuple(),\n",
        "                outline=\"blue\",\n",
        "                width=thickness,\n",
        "            )\n",
        "    for p in image_by_page:\n",
        "        img = image_by_page[p]\n",
        "        plt.figure(figsize=[15, 15])\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk9aIGifsSQK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}